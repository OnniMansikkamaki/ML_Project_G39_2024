{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team 39 Contributors:\n",
    "* 20241457 Onni Mansikkamäki\n",
    "* 20240506 Jan-Louis Schneider\n",
    "* 20240945 Emir Kamiloglu\n",
    "* 20240941 Tomás Figueiredo\n",
    "* 20240661 Rita Santos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summarytools in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from summarytools) (2.2.2)\n",
      "Requirement already satisfied: ipython>=7.20.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from summarytools) (8.27.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from summarytools) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from summarytools) (3.9.2)\n",
      "Requirement already satisfied: decorator in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=7.20.0->summarytools) (4.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from matplotlib>=3.3.0->summarytools) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from pandas>=1.4.0->summarytools) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from pandas>=1.4.0->summarytools) (2024.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.20.0->summarytools) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.20.0->summarytools) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.20.0->summarytools) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->summarytools) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from stack-data->ipython>=7.20.0->summarytools) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from stack-data->ipython>=7.20.0->summarytools) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from stack-data->ipython>=7.20.0->summarytools) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: annoy in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (1.17.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/onnimansikkamaki/opt/anaconda3/envs/ML/lib/python3.12/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "StandardScaler imported\n"
     ]
    }
   ],
   "source": [
    "%pip install summarytools\n",
    "%pip install ipywidgets\n",
    "%pip install annoy\n",
    "%pip install tensorflow\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "import phik\n",
    "import warnings\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "print(\"StandardScaler imported\")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "from summarytools import dfSummary\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats \n",
    "\n",
    "import joblib\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data\n",
    "\n",
    "with open(\"claim_injury_type_mapping.json\", \"r\") as file:\n",
    "    claim_injury_type_mapping = json.load(file)\n",
    "\n",
    "train_data_standardized = pd.read_csv(\"train_data_processed_standardized.csv\")\n",
    "train_data_non_standardized = pd.read_csv(\"train_data_processed_non_standardized.csv\")\n",
    "\n",
    "test_data_standardized = pd.read_csv(\"test_data_processed_standardized.csv\")\n",
    "test_data_non_standardized = pd.read_csv(\"test_data_processed_non_standardized.csv\")\n",
    "\n",
    "validation_data_standardized = pd.read_csv(\"validation_data_processed_standardized.csv\")\n",
    "validation_data_non_standardized = pd.read_csv(\"validation_data_processed_non_standardized.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Accident Date', 'Age at Injury', 'Assembly Date',\n",
      "       'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date',\n",
      "       'Carrier Type', 'Claim Injury Type', 'District Name', 'Gender',\n",
      "       'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code',\n",
      "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',\n",
      "       'Number of Dependents', 'Days Until Injury Reported'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>Carrier Type</th>\n",
       "      <th>Claim Injury Type</th>\n",
       "      <th>District Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Until Injury Reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.658501</td>\n",
       "      <td>-0.763726</td>\n",
       "      <td>-0.609139</td>\n",
       "      <td>0</td>\n",
       "      <td>0.668627</td>\n",
       "      <td>-0.581041</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839945</td>\n",
       "      <td>42</td>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "      <td>55</td>\n",
       "      <td>-1.501980</td>\n",
       "      <td>-0.268603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.923888</td>\n",
       "      <td>-0.763726</td>\n",
       "      <td>0.627758</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.496233</td>\n",
       "      <td>0.618894</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>33</td>\n",
       "      <td>98</td>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "      <td>-1.002320</td>\n",
       "      <td>1.136203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027909</td>\n",
       "      <td>-0.689781</td>\n",
       "      <td>0.138087</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.480410</td>\n",
       "      <td>0.151551</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "      <td>1.495978</td>\n",
       "      <td>-0.364385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.494884</td>\n",
       "      <td>-1.355279</td>\n",
       "      <td>1.664335</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.480410</td>\n",
       "      <td>1.670417</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.501980</td>\n",
       "      <td>-0.332457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.314540</td>\n",
       "      <td>1.676432</td>\n",
       "      <td>-1.318208</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.854524</td>\n",
       "      <td>-1.294687</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-1.002320</td>\n",
       "      <td>-0.194105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accident Date  Age at Injury  Assembly Date  Attorney/Representative  \\\n",
       "0      -0.658501      -0.763726      -0.609139                        0   \n",
       "1       0.923888      -0.763726       0.627758                        0   \n",
       "2       0.027909      -0.689781       0.138087                        0   \n",
       "3       1.494884      -1.355279       1.664335                        0   \n",
       "4      -1.314540       1.676432      -1.318208                        0   \n",
       "\n",
       "   Average Weekly Wage  C-2 Date  Carrier Type  Claim Injury Type  \\\n",
       "0             0.668627 -0.581041             1                  3   \n",
       "1            -0.496233  0.618894             1                  1   \n",
       "2            -0.480410  0.151551             2                  2   \n",
       "3            -0.480410  1.670417             1                  2   \n",
       "4            -0.854524 -1.294687             1                  3   \n",
       "\n",
       "   District Name  Gender  IME-4 Count  Industry Code  \\\n",
       "0              1       1     0.839945             42   \n",
       "1              6       2    -0.418975             33   \n",
       "2              6       1    -0.418975             62   \n",
       "3              3       1    -0.418975             62   \n",
       "4              8       1    -0.418975             44   \n",
       "\n",
       "   WCIO Cause of Injury Code  WCIO Nature of Injury Code  \\\n",
       "0                         25                          49   \n",
       "1                         98                          59   \n",
       "2                         19                          43   \n",
       "3                         45                           7   \n",
       "4                         56                          49   \n",
       "\n",
       "   WCIO Part Of Body Code  Number of Dependents  Days Until Injury Reported  \n",
       "0                      55             -1.501980                   -0.268603  \n",
       "1                      34             -1.002320                    1.136203  \n",
       "2                      37              1.495978                   -0.364385  \n",
       "3                      12             -1.501980                   -0.332457  \n",
       "4                      38             -1.002320                   -0.194105  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "print(train_data_standardized.columns)\n",
    "train_data_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Accident Date', 'Age at Injury', 'Assembly Date',\n",
      "       'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date',\n",
      "       'Carrier Type', 'Claim Injury Type', 'District Name', 'Gender',\n",
      "       'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code',\n",
      "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',\n",
      "       'Number of Dependents', 'Days Until Injury Reported'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>Carrier Type</th>\n",
       "      <th>Claim Injury Type</th>\n",
       "      <th>District Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Until Injury Reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1048</td>\n",
       "      <td>32</td>\n",
       "      <td>1039</td>\n",
       "      <td>0</td>\n",
       "      <td>1146.82</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42</td>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1569</td>\n",
       "      <td>32</td>\n",
       "      <td>1428</td>\n",
       "      <td>0</td>\n",
       "      <td>443.79</td>\n",
       "      <td>1422</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33</td>\n",
       "      <td>98</td>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1274</td>\n",
       "      <td>33</td>\n",
       "      <td>1274</td>\n",
       "      <td>0</td>\n",
       "      <td>453.34</td>\n",
       "      <td>1274</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1757</td>\n",
       "      <td>24</td>\n",
       "      <td>1754</td>\n",
       "      <td>0</td>\n",
       "      <td>453.34</td>\n",
       "      <td>1755</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>832</td>\n",
       "      <td>65</td>\n",
       "      <td>816</td>\n",
       "      <td>0</td>\n",
       "      <td>227.55</td>\n",
       "      <td>816</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accident Date  Age at Injury  Assembly Date  Attorney/Representative  \\\n",
       "0           1048             32           1039                        0   \n",
       "1           1569             32           1428                        0   \n",
       "2           1274             33           1274                        0   \n",
       "3           1757             24           1754                        0   \n",
       "4            832             65            816                        0   \n",
       "\n",
       "   Average Weekly Wage  C-2 Date  Carrier Type  Claim Injury Type  \\\n",
       "0              1146.82      1042             1                  3   \n",
       "1               443.79      1422             1                  1   \n",
       "2               453.34      1274             2                  2   \n",
       "3               453.34      1755             1                  2   \n",
       "4               227.55       816             1                  3   \n",
       "\n",
       "   District Name  Gender  IME-4 Count  Industry Code  \\\n",
       "0              1       1          2.0             42   \n",
       "1              6       2          0.0             33   \n",
       "2              6       1          0.0             62   \n",
       "3              3       1          0.0             62   \n",
       "4              8       1          0.0             44   \n",
       "\n",
       "   WCIO Cause of Injury Code  WCIO Nature of Injury Code  \\\n",
       "0                         25                          49   \n",
       "1                         98                          59   \n",
       "2                         19                          43   \n",
       "3                         45                           7   \n",
       "4                         56                          49   \n",
       "\n",
       "   WCIO Part Of Body Code  Number of Dependents  Days Until Injury Reported  \n",
       "0                      55                   0.0                           9  \n",
       "1                      34                   1.0                         141  \n",
       "2                      37                   6.0                           0  \n",
       "3                      12                   0.0                           3  \n",
       "4                      38                   1.0                          16  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data_non_standardized.columns)\n",
    "train_data_non_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Accident Date', 'Age at Injury', 'Assembly Date',\n",
      "       'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date',\n",
      "       'Carrier Type', 'District Name', 'Gender', 'IME-4 Count',\n",
      "       'Industry Code', 'WCIO Cause of Injury Code',\n",
      "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',\n",
      "       'Number of Dependents', 'Days Until Injury Reported'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>Carrier Type</th>\n",
       "      <th>District Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Until Injury Reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.685080</td>\n",
       "      <td>-1.725000</td>\n",
       "      <td>-1.683872</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.178122</td>\n",
       "      <td>-1.657825</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>48</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>-1.002320</td>\n",
       "      <td>-0.268603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.581815</td>\n",
       "      <td>-1.725000</td>\n",
       "      <td>-1.683872</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.793964</td>\n",
       "      <td>-1.657825</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.002320</td>\n",
       "      <td>0.093241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.691155</td>\n",
       "      <td>1.232767</td>\n",
       "      <td>-1.683872</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.231555</td>\n",
       "      <td>-1.651510</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>56</td>\n",
       "      <td>68</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.501980</td>\n",
       "      <td>-0.289887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.697229</td>\n",
       "      <td>0.936991</td>\n",
       "      <td>-1.683872</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.231555</td>\n",
       "      <td>-1.657825</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>1.495978</td>\n",
       "      <td>-0.311172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.672931</td>\n",
       "      <td>-1.281335</td>\n",
       "      <td>-1.683872</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.231555</td>\n",
       "      <td>-1.651510</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>0.996319</td>\n",
       "      <td>-0.226033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accident Date  Age at Injury  Assembly Date  Attorney/Representative  \\\n",
       "0      -1.685080      -1.725000      -1.683872                        0   \n",
       "1      -1.581815      -1.725000      -1.683872                        0   \n",
       "2      -1.691155       1.232767      -1.683872                        0   \n",
       "3      -1.697229       0.936991      -1.683872                        0   \n",
       "4      -1.672931      -1.281335      -1.683872                        0   \n",
       "\n",
       "   Average Weekly Wage  C-2 Date  Carrier Type  District Name  Gender  \\\n",
       "0            -0.178122 -1.657825             1              5       1   \n",
       "1            -0.793964 -1.657825             1              5       2   \n",
       "2            -1.231555 -1.651510             1              5       2   \n",
       "3            -1.231555 -1.657825             1              5       2   \n",
       "4            -1.231555 -1.651510             1              5       1   \n",
       "\n",
       "   IME-4 Count  Industry Code  WCIO Cause of Injury Code  \\\n",
       "0    -0.418975             48                         31   \n",
       "1    -0.418975             45                         75   \n",
       "2    -0.418975             56                         68   \n",
       "3    -0.418975             48                         25   \n",
       "4    -0.418975             55                         79   \n",
       "\n",
       "   WCIO Nature of Injury Code  WCIO Part Of Body Code  Number of Dependents  \\\n",
       "0                          10                      54             -1.002320   \n",
       "1                          10                      10             -1.002320   \n",
       "2                          49                      62             -1.501980   \n",
       "3                          10                      53              1.495978   \n",
       "4                          40                      37              0.996319   \n",
       "\n",
       "   Days Until Injury Reported  \n",
       "0                   -0.268603  \n",
       "1                    0.093241  \n",
       "2                   -0.289887  \n",
       "3                   -0.311172  \n",
       "4                   -0.226033  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data_standardized.columns)\n",
    "test_data_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Accident Date', 'Age at Injury', 'Assembly Date',\n",
      "       'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date',\n",
      "       'Carrier Type', 'District Name', 'Gender', 'IME-4 Count',\n",
      "       'Industry Code', 'WCIO Cause of Injury Code',\n",
      "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',\n",
      "       'Number of Dependents', 'Days Until Injury Reported'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>Carrier Type</th>\n",
       "      <th>District Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Until Injury Reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>710</td>\n",
       "      <td>19</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>635.78</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>744</td>\n",
       "      <td>19</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>264.10</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>708</td>\n",
       "      <td>59</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>703</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56</td>\n",
       "      <td>68</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>706</td>\n",
       "      <td>55</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>714</td>\n",
       "      <td>25</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>703</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accident Date  Age at Injury  Assembly Date  Attorney/Representative  \\\n",
       "0            710             19            701                        0   \n",
       "1            744             19            701                        0   \n",
       "2            708             59            701                        0   \n",
       "3            706             55            701                        0   \n",
       "4            714             25            701                        0   \n",
       "\n",
       "   Average Weekly Wage  C-2 Date  Carrier Type  District Name  Gender  \\\n",
       "0               635.78       701             1              5       1   \n",
       "1               264.10       701             1              5       2   \n",
       "2                 0.00       703             1              5       2   \n",
       "3                 0.00       701             1              5       2   \n",
       "4                 0.00       703             1              5       1   \n",
       "\n",
       "   IME-4 Count  Industry Code  WCIO Cause of Injury Code  \\\n",
       "0          0.0             48                         31   \n",
       "1          0.0             45                         75   \n",
       "2          0.0             56                         68   \n",
       "3          0.0             48                         25   \n",
       "4          0.0             55                         79   \n",
       "\n",
       "   WCIO Nature of Injury Code  WCIO Part Of Body Code  Number of Dependents  \\\n",
       "0                          10                      54                   1.0   \n",
       "1                          10                      10                   1.0   \n",
       "2                          49                      62                   0.0   \n",
       "3                          10                      53                   6.0   \n",
       "4                          40                      37                   5.0   \n",
       "\n",
       "   Days Until Injury Reported  \n",
       "0                           9  \n",
       "1                          43  \n",
       "2                           7  \n",
       "3                           5  \n",
       "4                          13  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data_non_standardized.columns)\n",
    "test_data_non_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Accident Date', 'Age at Injury', 'Assembly Date',\n",
      "       'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date',\n",
      "       'Carrier Type', 'Claim Injury Type', 'District Name', 'Gender',\n",
      "       'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code',\n",
      "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',\n",
      "       'Number of Dependents', 'Days Until Injury Reported'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>Carrier Type</th>\n",
       "      <th>Claim Injury Type</th>\n",
       "      <th>District Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Until Injury Reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.392375</td>\n",
       "      <td>0.863046</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.758655</td>\n",
       "      <td>0.214706</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>61</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>0.996319</td>\n",
       "      <td>0.103884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.434896</td>\n",
       "      <td>-1.429223</td>\n",
       "      <td>0.519649</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.480410</td>\n",
       "      <td>0.530478</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>62</td>\n",
       "      <td>74</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>0.496659</td>\n",
       "      <td>-0.215390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.595112</td>\n",
       "      <td>-0.467949</td>\n",
       "      <td>1.740647</td>\n",
       "      <td>0</td>\n",
       "      <td>1.164641</td>\n",
       "      <td>1.743045</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>62</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>-0.236675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.153567</td>\n",
       "      <td>0.789102</td>\n",
       "      <td>-1.127427</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.793964</td>\n",
       "      <td>-1.095750</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>52</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>-0.268603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.577645</td>\n",
       "      <td>1.010935</td>\n",
       "      <td>0.608680</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.480410</td>\n",
       "      <td>0.618894</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.418975</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.502661</td>\n",
       "      <td>-0.013183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accident Date  Age at Injury  Assembly Date  Attorney/Representative  \\\n",
       "0       0.392375       0.863046       0.379743                        0   \n",
       "1       0.434896      -1.429223       0.519649                        0   \n",
       "2       1.595112      -0.467949       1.740647                        0   \n",
       "3      -1.153567       0.789102      -1.127427                        0   \n",
       "4       0.577645       1.010935       0.608680                        0   \n",
       "\n",
       "   Average Weekly Wage  C-2 Date  Carrier Type  Claim Injury Type  \\\n",
       "0            -0.758655  0.214706             3                  2   \n",
       "1            -0.480410  0.530478             4                  2   \n",
       "2             1.164641  1.743045             1                  4   \n",
       "3            -0.793964 -1.095750             2                  2   \n",
       "4            -0.480410  0.618894             4                  2   \n",
       "\n",
       "   District Name  Gender  IME-4 Count  Industry Code  \\\n",
       "0              5       2    -0.418975             61   \n",
       "1              1       2    -0.418975             62   \n",
       "2              3       2    -0.418975             62   \n",
       "3              2       1    -0.418975             45   \n",
       "4              5       2    -0.418975             62   \n",
       "\n",
       "   WCIO Cause of Injury Code  WCIO Nature of Injury Code  \\\n",
       "0                         74                          10   \n",
       "1                         74                          52   \n",
       "2                         50                          49   \n",
       "3                         60                          52   \n",
       "4                         19                          43   \n",
       "\n",
       "   WCIO Part Of Body Code  Number of Dependents  Days Until Injury Reported  \n",
       "0                      35              0.996319                    0.103884  \n",
       "1                      53              0.496659                   -0.215390  \n",
       "2                      38             -0.003001                   -0.236675  \n",
       "3                      35             -0.003001                   -0.268603  \n",
       "4                      36             -0.502661                   -0.013183  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(validation_data_standardized.columns)\n",
    "validation_data_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Accident Date', 'Age at Injury', 'Assembly Date',\n",
      "       'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date',\n",
      "       'Carrier Type', 'Claim Injury Type', 'District Name', 'Gender',\n",
      "       'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code',\n",
      "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',\n",
      "       'Number of Dependents', 'Days Until Injury Reported'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>Carrier Type</th>\n",
       "      <th>Claim Injury Type</th>\n",
       "      <th>District Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Days Until Injury Reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1394</td>\n",
       "      <td>54</td>\n",
       "      <td>1350</td>\n",
       "      <td>0</td>\n",
       "      <td>285.41</td>\n",
       "      <td>1294</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1408</td>\n",
       "      <td>23</td>\n",
       "      <td>1394</td>\n",
       "      <td>0</td>\n",
       "      <td>453.34</td>\n",
       "      <td>1394</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>74</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1790</td>\n",
       "      <td>36</td>\n",
       "      <td>1778</td>\n",
       "      <td>0</td>\n",
       "      <td>1446.18</td>\n",
       "      <td>1778</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>885</td>\n",
       "      <td>53</td>\n",
       "      <td>876</td>\n",
       "      <td>0</td>\n",
       "      <td>264.10</td>\n",
       "      <td>879</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>52</td>\n",
       "      <td>35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1455</td>\n",
       "      <td>56</td>\n",
       "      <td>1422</td>\n",
       "      <td>0</td>\n",
       "      <td>453.34</td>\n",
       "      <td>1422</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accident Date  Age at Injury  Assembly Date  Attorney/Representative  \\\n",
       "0           1394             54           1350                        0   \n",
       "1           1408             23           1394                        0   \n",
       "2           1790             36           1778                        0   \n",
       "3            885             53            876                        0   \n",
       "4           1455             56           1422                        0   \n",
       "\n",
       "   Average Weekly Wage  C-2 Date  Carrier Type  Claim Injury Type  \\\n",
       "0               285.41      1294             3                  2   \n",
       "1               453.34      1394             4                  2   \n",
       "2              1446.18      1778             1                  4   \n",
       "3               264.10       879             2                  2   \n",
       "4               453.34      1422             4                  2   \n",
       "\n",
       "   District Name  Gender  IME-4 Count  Industry Code  \\\n",
       "0              5       2          0.0             61   \n",
       "1              1       2          0.0             62   \n",
       "2              3       2          0.0             62   \n",
       "3              2       1          0.0             45   \n",
       "4              5       2          0.0             62   \n",
       "\n",
       "   WCIO Cause of Injury Code  WCIO Nature of Injury Code  \\\n",
       "0                         74                          10   \n",
       "1                         74                          52   \n",
       "2                         50                          49   \n",
       "3                         60                          52   \n",
       "4                         19                          43   \n",
       "\n",
       "   WCIO Part Of Body Code  Number of Dependents  Days Until Injury Reported  \n",
       "0                      35                   5.0                          44  \n",
       "1                      53                   4.0                          14  \n",
       "2                      38                   3.0                          12  \n",
       "3                      35                   3.0                           9  \n",
       "4                      36                   2.0                          33  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(validation_data_non_standardized.columns)\n",
    "validation_data_non_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### KNN Classifier with PCA to reduce dimensionality of data.\n",
    "\n",
    " * Normal KNN, it took so much time that for training, data was sampled from training data in the beginning. Later more data was added to training\n",
    "\n",
    " * Approximation of NN calculation was used so that the calculation would be faster\n",
    "\n",
    "     * Annoy library (Approximate Nearest Neighbors Oh Yeah) was used to get the approximation of the NN\n",
    "\n",
    "         * Annoy algorithm builds multiple trees for fast approximate neighbor search. A higher number of trees increases accuracy but slows down building of the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# #KNN with PCA, data sampling Annoy optimization\n",
    "\n",
    "# train_data_knn = train_data.copy()\n",
    "# test_data_knn = test_data.copy()\n",
    "\n",
    "# # Define the independant variables (as data) and the target variable\n",
    "# data_knn = train_data_knn.drop([\"Claim Injury Type\"], axis=1)\n",
    "# target_knn = train_data_knn[\"Claim Injury Type\"]\n",
    "\n",
    "# # Split data to X_train, X_test, y_train and y_test\n",
    "# X_train_knn, X_val_knn, y_train_knn, y_val_knn = train_test_split(data_knn, \n",
    "#                                                   target_knn, \n",
    "#                                                   test_size = 0.2, \n",
    "#                                                   random_state=5, \n",
    "#                                                   stratify = target_knn)\n",
    "\n",
    "\n",
    "# # Sample and reduce dimensionality of the data\n",
    "# sample_size = 400000\n",
    "# X_train_knn_sample, _, y_train_knn_sample, _ = train_test_split(X_train_knn, y_train_knn, train_size=sample_size, random_state=42)\n",
    "# # Determine the number of PC's based on available features or samples\n",
    "# n_components = min(15, X_train_knn_sample.shape[1])\n",
    "# pca = PCA(n_components=n_components)\n",
    "# X_train_knn_reduced = pca.fit_transform(X_train_knn_sample)\n",
    "# X_val_knn_reduced = pca.transform(X_val_knn)\n",
    "\n",
    "# # Build Annoy index with reduced data\n",
    "# index = AnnoyIndex(X_train_knn_reduced.shape[1], 'euclidean')\n",
    "# for i, vector in enumerate(X_train_knn_reduced):\n",
    "#     index.add_item(i, vector)\n",
    "# index.build(10)  # Number of trees in Annoy; higher means better accuracy but slower build\n",
    "\n",
    "\n",
    "# # Convert y_train_knn_sample to a NumPy array for positional indexing\n",
    "# y_train_knn_sample = y_train_knn_sample.to_numpy()\n",
    "\n",
    "# # Function to get predictions using Annoy \n",
    "# '''\n",
    "# Retrieves the indices of the k nearest neighbors for the vector x in X_val. These are approximate neighbors based on the trees built by Annoy.\n",
    "# '''\n",
    "\n",
    "# def get_approximate_knn_predictions(X_val, index, y_train, k):\n",
    "#     predictions = []\n",
    "#     for x in X_val:\n",
    "#         nearest_indices = index.get_nns_by_vector(x, k)\n",
    "#         nearest_labels = [y_train[idx] for idx in nearest_indices]\n",
    "#         predictions.append(max(set(nearest_labels), key=nearest_labels.count))\n",
    "#     return predictions\n",
    "\n",
    "# # Evaluate each range of k values\n",
    "# k_ranges = {\n",
    "#     \"2-10\": range(2, 11),\n",
    "#     \"11-20\": range(11, 21),\n",
    "#     \"21-30\": range(21, 31)\n",
    "#     # \"100-110\": range(100, 111)\n",
    "# }\n",
    "\n",
    "# best_k_values = {}\n",
    "# best_accuracy_scores = {}\n",
    "# best_f1_scores = {}\n",
    "# best_predictions = {}\n",
    "\n",
    "# for label, k_range in k_ranges.items():\n",
    "#     accuracy_scores = []\n",
    "#     f1_scores = []\n",
    "#     high_accuracy = 0\n",
    "#     best_k = 0\n",
    "#     best_preds = None\n",
    "#     best_f1 = 0  # Track F1 score for the best k based on accuracy\n",
    "\n",
    "#     print(f\"Evaluating k values in range: {label}\")\n",
    "    \n",
    "#     for k in k_range:\n",
    "#         # Get approximate KNN predictions\n",
    "#         predictions = get_approximate_knn_predictions(X_val_knn_reduced, index, y_train_knn_sample, k)\n",
    "        \n",
    "#         # Calculate accuracy and F1 score\n",
    "#         accuracy = accuracy_score(y_val_knn, predictions)\n",
    "#         f1 = f1_score(y_val_knn, predictions, average='weighted')\n",
    "        \n",
    "#         accuracy_scores.append(accuracy)\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#         # Update best k if this model has a higher accuracy\n",
    "#         if accuracy > high_accuracy:\n",
    "#             high_accuracy = accuracy\n",
    "#             best_k = k\n",
    "#             best_preds = predictions\n",
    "#             best_f1 = f1  \n",
    "\n",
    "#     # Store results for this range\n",
    "#     best_k_values[label] = best_k\n",
    "#     best_accuracy_scores[label] = high_accuracy\n",
    "#     best_f1_scores[label] = best_f1\n",
    "#     best_predictions[label] = best_preds\n",
    "\n",
    "#     # Plot Accuracy and F1 Score for the current k range\n",
    "#     plt.figure()\n",
    "#     plt.plot(list(k_range), accuracy_scores, marker='o', label='Accuracy')\n",
    "#     plt.plot(list(k_range), f1_scores, marker='x', label='F1 Score')\n",
    "#     plt.xlabel('Number of Neighbors (k)')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title(f'Accuracy and F1 Score for k values in Range {label}')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Output the best result for the range\n",
    "#     print(f\"Best k in range {label}: {best_k}\")\n",
    "#     print(f\"Best Accuracy in range {label}: {high_accuracy}\")\n",
    "#     print(f\"F1 Score for best k ({best_k}) in range {label}: {best_f1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # We saw that with k over 21, we are not gaining accuracy or f1 score anymore. Let's now try more interesting k-values under 21 and also see the training data accuracy to see if we are overfitting.\n",
    "\n",
    "\n",
    "\n",
    " # We found that training and evaluation accuracy are getting closer to each other without losing val accuracy with under k<20. Let's try also k values over 20 and see if we can reduce overfitting without losing too much f1 score and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# train_data_knn = train_data.copy()\n",
    "# test_data_knn = test_data.copy()\n",
    "\n",
    "# # Define the independant variables (as data) and the target variable\n",
    "# data_knn = train_data_knn.drop([\"Claim Injury Type\"], axis=1)\n",
    "# target_knn = train_data_knn[\"Claim Injury Type\"]\n",
    "\n",
    "# # Split data to X_train, X_test, y_train and y_test\n",
    "\n",
    "# X_train_knn, X_val_knn, y_train_knn, y_val_knn = train_test_split(data_knn, \n",
    "#                                                   target_knn, \n",
    "#                                                   test_size = 0.2, \n",
    "#                                                   random_state=5, \n",
    "#                                                   stratify = target_knn)\n",
    "\n",
    "# # Sample and reduce dimensionality of the data\n",
    "# sample_size = 400000\n",
    "# X_train_knn_sample, _, y_train_knn_sample, _ = train_test_split(X_train_knn, y_train_knn, train_size=sample_size, random_state=42)\n",
    "# pca = PCA(n_components=min(15, X_train_knn_sample.shape[1]))  # Adjusting PCA components as needed\n",
    "# X_train_knn_reduced = pca.fit_transform(X_train_knn_sample)\n",
    "# X_val_knn_reduced = pca.transform(X_val_knn)\n",
    "\n",
    "# # Convert y_train_knn_sample to a NumPy array for reliable indexing\n",
    "# y_train_knn_sample = y_train_knn_sample.to_numpy()\n",
    "\n",
    "# #  Build Annoy index with reduced data\n",
    "# index = AnnoyIndex(X_train_knn_reduced.shape[1], 'euclidean')\n",
    "# for i, vector in enumerate(X_train_knn_reduced):\n",
    "#     index.add_item(i, vector)\n",
    "# index.build(10)  \n",
    "\n",
    "# # Function to get predictions using Annoy\n",
    "# def get_approximate_knn_predictions(X_val, index, y_train, k):\n",
    "#     predictions = []\n",
    "#     for x in X_val:\n",
    "#         nearest_indices = index.get_nns_by_vector(x, k)\n",
    "#         nearest_labels = [y_train[idx] for idx in nearest_indices]\n",
    "#         predictions.append(max(set(nearest_labels), key=nearest_labels.count))\n",
    "#     return predictions\n",
    "\n",
    "# # Evaluate each range of k values\n",
    "\n",
    "# k_ranges = {\n",
    "#     \"2-10\": range(2, 11),\n",
    "#     \"11-20\": range(11, 21),\n",
    "#     \"21-40\": range(21, 41),\n",
    "# }\n",
    "\n",
    "# best_k_values = {}\n",
    "# best_accuracy_scores = {}\n",
    "# best_f1_scores = {}\n",
    "# best_predictions = {}\n",
    "\n",
    "# for label, k_range in k_ranges.items():\n",
    "#     accuracy_scores_train = []\n",
    "#     accuracy_scores_val = []\n",
    "#     f1_scores = []\n",
    "#     high_accuracy = 0\n",
    "#     best_k = 0\n",
    "#     best_preds = None\n",
    "#     best_f1 = 0  # Track F1 score for the best k based on validation accuracy\n",
    "\n",
    "#     print(f\"Evaluating k values in range: {label}\")\n",
    "    \n",
    "#     for k in k_range:\n",
    "#         # Get predictions and accuracy for the validation data\n",
    "#         predictions_val = get_approximate_knn_predictions(X_val_knn_reduced, index, y_train_knn_sample, k)\n",
    "#         accuracy_val = accuracy_score(y_val_knn, predictions_val)\n",
    "#         f1_val = f1_score(y_val_knn, predictions_val, average='weighted')\n",
    "\n",
    "#         # Get predictions and accuracy for the training data\n",
    "#         predictions_train = get_approximate_knn_predictions(X_train_knn_reduced, index, y_train_knn_sample, k)\n",
    "#         accuracy_train = accuracy_score(y_train_knn_sample, predictions_train)\n",
    "\n",
    "#         # Store training and validation accuracy and F1 score\n",
    "#         accuracy_scores_train.append(accuracy_train)\n",
    "#         accuracy_scores_val.append(accuracy_val)\n",
    "#         f1_scores.append(f1_val)\n",
    "\n",
    "#         # Update best k if this model has a higher validation accuracy\n",
    "#         if accuracy_val > high_accuracy:\n",
    "#             high_accuracy = accuracy_val\n",
    "#             best_k = k\n",
    "#             best_preds = predictions_val\n",
    "#             best_f1 = f1_val  # Store the F1 score for the best accuracy on validation data\n",
    "\n",
    "#     # Store results for this range\n",
    "#     best_k_values[label] = best_k\n",
    "#     best_accuracy_scores[label] = high_accuracy\n",
    "#     best_f1_scores[label] = best_f1\n",
    "#     best_predictions[label] = best_preds\n",
    "\n",
    "#     # Plot Accuracy (Train and Validation) and F1 Score for the current k range\n",
    "#     plt.figure()\n",
    "#     plt.plot(list(k_range), accuracy_scores_train, marker='o', label='Training Accuracy')\n",
    "#     plt.plot(list(k_range), accuracy_scores_val, marker='x', label='Validation Accuracy')\n",
    "#     plt.plot(list(k_range), f1_scores, marker='^', label='Validation F1 Score')\n",
    "#     plt.xlabel('Number of Neighbors (k)')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title(f'Accuracy and F1 Score for k values in Range {label}')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Output the best result for the range\n",
    "#     print(f\"Best k in range {label}: {best_k}\")\n",
    "#     print(f\"Best Validation Accuracy in range {label}: {high_accuracy}\")\n",
    "#     print(f\"F1 Score for best k ({best_k}) in range {label}: {best_f1}\")\n",
    "#     print(f\"Training Accuracy for best k ({best_k}) in range {label}: {accuracy_scores_train[k_range.index(best_k)]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Best K taking accuracy in both val and test set in account seems to be k=35. Even though training accuracy is dropping, the overfitting is reduced with higher k. Validation performance is staying quite consistent with higher k and f1 is not dropping significantly. Later in optimization we might want to oversample training data that has the minority class value of the traget variable. Now our model is probably not good in classification of those minority target variable class rows.\n",
    "\n",
    "\n",
    "\n",
    " ## Even though best validation accuracy is found with k=24, I prefer to have better generalizing model (less overfitting) than only 0.1% better validation accuracy. Higher k than 35 is not chosen due to efficiency reasons at this point. Maybe later with more time in our hands we could go for higher k.\n",
    "\n",
    "\n",
    "\n",
    " ## Even with k=35, model is slightly overfitting still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Transform the test data with PCA\n",
    "# X_test_knn_reduced = pca.transform(test_data_knn) \n",
    "\n",
    "# # Make predictions on the transformed test data using the best k value\n",
    "# best_k = 35 # Replace with the range key that had the best k (35)\n",
    "# test_predictions = get_approximate_knn_predictions(X_test_knn_reduced, index, y_train_knn_sample, best_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# test_predictions = [int(pred) for pred in test_predictions]\n",
    "\n",
    "# print(test_predictions)\n",
    "# print(claim_injury_type_mapping)\n",
    "\n",
    "# # Format the predictions according to the required label format\n",
    "\n",
    "# # # Convert numerical predictions to the required string format using label mapping\n",
    "# formatted_predictions = [claim_injury_type_mapping[pred] for pred in test_predictions]\n",
    "\n",
    "# formatted_predictions\n",
    "\n",
    "# # # Create the submission DataFrame\n",
    "# submission = pd.DataFrame({\n",
    "#     \"Claim Identifier\": test_data.index,\n",
    "#     \"Claim Injury Type\": formatted_predictions\n",
    "# })\n",
    "\n",
    "# submission.head()\n",
    "# # # Save to CSV in the required format\n",
    "# submission.to_csv(\"model1.csv\", index=False)\n",
    "\n",
    "# print(\"Submission file created: model1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Experiment with standardization for also categorial predictor variables (MODEL FOR BEST KAGGLE RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# train_data_knn = train_data_non_standardized.copy()\n",
    "# test_data_knn = test_data_non_standardized.copy()\n",
    "\n",
    "# # Get all columns from the train data which are not the target variable\n",
    "# predictor_cols = train_data_knn.columns[train_data_knn.columns != \"Claim Injury Type\"]\n",
    "\n",
    "# # Initialize the StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(train_data_knn[predictor_cols])\n",
    "\n",
    "# # Scale the columns in both train and test data\n",
    "# train_data_knn[predictor_cols] = scaler.transform(train_data_knn[predictor_cols])\n",
    "# test_data_knn[predictor_cols] = scaler.transform(test_data_knn[predictor_cols])\n",
    "\n",
    "\n",
    "# # Define the independant variables (as data) and the target variable\n",
    "# data_knn = train_data_knn.drop([\"Claim Injury Type\"], axis=1)\n",
    "# target_knn = train_data_knn[\"Claim Injury Type\"]\n",
    "\n",
    "# # Split data to X_train, X_test, y_train and y_test\n",
    "# X_train_knn, X_val_knn, y_train_knn, y_val_knn = train_test_split(data_knn, \n",
    "#                                                   target_knn, \n",
    "#                                                   test_size = 0.2, \n",
    "#                                                   random_state=5, \n",
    "#                                                   stratify = target_knn)\n",
    "\n",
    "# # Sample and reduce dimensionality of the data\n",
    "# sample_size = 400000\n",
    "# X_train_knn_sample, _, y_train_knn_sample, _ = train_test_split(X_train_knn, y_train_knn, train_size=sample_size, random_state=42)\n",
    "# pca = PCA(n_components=min(15, X_train_knn_sample.shape[1]))  # Adjusting PCA components as needed\n",
    "# X_train_knn_reduced = pca.fit_transform(X_train_knn_sample)\n",
    "# X_val_knn_reduced = pca.transform(X_val_knn)\n",
    "\n",
    "# # Convert y_train_knn_sample to a NumPy array for reliable indexing\n",
    "# y_train_knn_sample = y_train_knn_sample.to_numpy()\n",
    "\n",
    "# #  Build Annoy index with reduced data\n",
    "# index = AnnoyIndex(X_train_knn_reduced.shape[1], 'euclidean')\n",
    "# for i, vector in enumerate(X_train_knn_reduced):\n",
    "#     index.add_item(i, vector)\n",
    "# index.build(10)  \n",
    "\n",
    "# # Function to get predictions using Annoy\n",
    "# def get_approximate_knn_predictions(X_val, index, y_train, k):\n",
    "#     predictions = []\n",
    "#     for x in X_val:\n",
    "#         nearest_indices = index.get_nns_by_vector(x, k)\n",
    "#         nearest_labels = [y_train[idx] for idx in nearest_indices]\n",
    "#         predictions.append(max(set(nearest_labels), key=nearest_labels.count))\n",
    "#     return predictions\n",
    "\n",
    "# # Evaluate each range of k values\n",
    "# k_ranges = {\n",
    "#     \"30-40\": range(30, 41),\n",
    "# }\n",
    "\n",
    "# best_k_values = {}\n",
    "# best_accuracy_scores = {}\n",
    "# best_f1_scores = {}\n",
    "# best_predictions = {}\n",
    "\n",
    "# for label, k_range in k_ranges.items():\n",
    "#     accuracy_scores_train = []\n",
    "#     accuracy_scores_val = []\n",
    "#     f1_scores = []\n",
    "#     high_accuracy = 0\n",
    "#     best_k = 0\n",
    "#     best_preds = None\n",
    "#     best_f1 = 0  # Track F1 score for the best k based on validation accuracy\n",
    "\n",
    "#     print(f\"Evaluating k values in range: {label}\")\n",
    "    \n",
    "#     for k in k_range:\n",
    "#         # Get predictions and accuracy for the validation data\n",
    "#         predictions_val = get_approximate_knn_predictions(X_val_knn_reduced, index, y_train_knn_sample, k)\n",
    "#         accuracy_val = accuracy_score(y_val_knn, predictions_val)\n",
    "#         f1_val = f1_score(y_val_knn, predictions_val, average='weighted')\n",
    "\n",
    "#         # Get predictions and accuracy for the training data\n",
    "#         predictions_train = get_approximate_knn_predictions(X_train_knn_reduced, index, y_train_knn_sample, k)\n",
    "#         accuracy_train = accuracy_score(y_train_knn_sample, predictions_train)\n",
    "\n",
    "#         # Store training and validation accuracy and F1 score\n",
    "#         accuracy_scores_train.append(accuracy_train)\n",
    "#         accuracy_scores_val.append(accuracy_val)\n",
    "#         f1_scores.append(f1_val)\n",
    "\n",
    "#         # Update best k if this model has a higher validation accuracy\n",
    "#         if accuracy_val > high_accuracy:\n",
    "#             high_accuracy = accuracy_val\n",
    "#             best_k = k\n",
    "#             best_preds = predictions_val\n",
    "#             best_f1 = f1_val  # Store the F1 score for the best accuracy on validation data\n",
    "\n",
    "#     # Store results for this range\n",
    "#     best_k_values[label] = best_k\n",
    "#     best_accuracy_scores[label] = high_accuracy\n",
    "#     best_f1_scores[label] = best_f1\n",
    "#     best_predictions[label] = best_preds\n",
    "\n",
    "#     # Plot Accuracy (Train and Validation) and F1 Score for the current k range\n",
    "#     plt.figure()\n",
    "#     plt.plot(list(k_range), accuracy_scores_train, marker='o', label='Training Accuracy')\n",
    "#     plt.plot(list(k_range), accuracy_scores_val, marker='x', label='Validation Accuracy')\n",
    "#     plt.plot(list(k_range), f1_scores, marker='^', label='Validation F1 Score')\n",
    "#     plt.xlabel('Number of Neighbors (k)')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title(f'Accuracy and F1 Score for k values in Range {label}')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Output the best result for the range\n",
    "#     print(f\"Best k in range {label}: {best_k}\")\n",
    "#     print(f\"Best Validation Accuracy in range {label}: {high_accuracy}\")\n",
    "#     print(f\"F1 Score for best k ({best_k}) in range {label}: {best_f1}\")\n",
    "#     print(f\"Training Accuracy for best k ({best_k}) in range {label}: {accuracy_scores_train[k_range.index(best_k)]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Best k seems to be 37 in this case from range 30 to 40.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Transform the test data with PCA\n",
    "# X_test_knn_reduced = pca.transform(test_data_knn) \n",
    "\n",
    "# # Make predictions on the transformed test data using the best k value\n",
    "# best_k = 37 # Replace with the range key that had the best k (37)\n",
    "# test_predictions = get_approximate_knn_predictions(X_test_knn_reduced, index, y_train_knn_sample, best_k)\n",
    "\n",
    "# test_predictions = [int(pred) for pred in test_predictions]\n",
    "\n",
    "# # Format the predictions according to the required label format\n",
    "\n",
    "# # # Convert numerical predictions to the required string format using label mapping\n",
    "# formatted_predictions = [claim_injury_type_mapping[pred] for pred in test_predictions]\n",
    "\n",
    "# formatted_predictions\n",
    "\n",
    "# # # Create the submission DataFrame\n",
    "# submission = pd.DataFrame({\n",
    "#     \"Claim Identifier\": test_data.index,\n",
    "#     \"Claim Injury Type\": formatted_predictions\n",
    "# })\n",
    "\n",
    "# submission.head()\n",
    "# # # Save to CSV in the required format\n",
    "# submission.to_csv(\"model4.csv\", index=False)\n",
    "\n",
    "# print(\"Submission file created: model4.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Optimizations and results\n",
    "\n",
    " * **Increase training data points (from sample of 50 000 to all of the data)**\n",
    "\n",
    "     * With 50 000 samples of training data\n",
    "\n",
    "         * **We got f1 score of 0.23926 in Kaggle**\n",
    "\n",
    "         * Best k in range 10-30 was k=20\n",
    "\n",
    "         * Best Validation Accuracy for k=20, 0.56\n",
    "\n",
    "         * F1 Score (for validation data) for k=20: 0.49\n",
    "\n",
    "         * Training data Accuracy for k = 20: 0.59382\n",
    "\n",
    "     * Increasing the sample size of training data for training to be 400 000 (sample of 400 000 to all of the data)\n",
    "\n",
    "         * **We got f1 score of 0.26029 in Kaggle**\n",
    "\n",
    "         * Best k in range 10-40 was k=35\n",
    "\n",
    "         * Best Validation Accuracy for k=35, approx. 0.595\n",
    "\n",
    "         * F1 Score (for validation data) for k=35: 0.5375\n",
    "\n",
    "         * Training data Accuracy for k = 35: 0.6175\n",
    "\n",
    "\n",
    "\n",
    " * **Scale categorial features as well**\n",
    "\n",
    "     * Let's use standardization to also standardize the categorial values instead of only metric columns.\n",
    "\n",
    "         * We will experience this with size of training data as 400 000 (sample of 400 000 to all of the data)\n",
    "\n",
    "         * Let's try 10 different k values around the k=35 (so from k=30 to k=40)\n",
    "\n",
    "         * Results:\n",
    "\n",
    "             * **We got f1 score of 0.27128 in Kaggle**\n",
    "\n",
    "             * Best k in range 30-40: 37\n",
    "\n",
    "             * Best Validation Accuracy k=37: 0.648\n",
    "\n",
    "             * F1 Score for k=37: 0.591\n",
    "\n",
    "             * Training Accuracy for k=37 in range 30-40: 0.659315\n",
    "\n",
    "\n",
    "\n",
    " ### Future optimizations for project part 2\n",
    "\n",
    " * **Let go more features**\n",
    "\n",
    "     * Get rid of highly correlating features with each other (WCIO... features)\n",
    "\n",
    "     * Add Average Weekly Wage as it might be important\n",
    "\n",
    "     * Try optimal features from LogisticRegression's RFE.\n",
    "\n",
    "     * Implement KNNClassifier with RFE\n",
    "\n",
    " * Oversample minority target variable values\n",
    "\n",
    " * Different PCA values or without PCA\n",
    "\n",
    " * Weighting closer neighbors more\n",
    "\n",
    " * KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Testing of almost random Classifier\n",
    "\n",
    " * Just for fun of it, random classifier was used. Guessing was weighted to favor most usual value in target variable in training data. Only other value we allowed the model to guess was the second most usual value in target variable in training data.\n",
    "\n",
    "\n",
    "\n",
    " ## Result: Bad as expected\n",
    "\n",
    " * In kaggle, f1 score of 0.13 was achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# NOTE: Commented out since this is not needed for achieving our final results\n",
    "\n",
    "# Find two most usual target values\n",
    "# print(train_data['Claim Injury Type'].value_counts())\n",
    "\n",
    "# Check label mapping\n",
    "# print(claim_injury_type_mapping)\n",
    "\n",
    "# Take the Claim Identifier numbers from model1.csv\n",
    "# model1 = pd.read_csv('model1.csv')\n",
    "# print(model1['Claim Identifier'])\n",
    "\n",
    "# # Make a new submission file (model2.csv) with the two most usual target values randomly assigned for every Claim Identifier in model1.csv wtih more ephasis for the most usual value\n",
    "\n",
    "\n",
    "# # Create a list of the two most usual target values\n",
    "# most_usual_values = [0, 1]\n",
    "\n",
    "# # Generate random guesses with weighted probabilities\n",
    "\n",
    "# # Define the probabilities for each value\n",
    "# probabilities = [0.8, 0.2]\n",
    "\n",
    "\n",
    "# print(label_mapping)\n",
    "\n",
    "# # Generate random guesses\n",
    "# random_guesses = np.random.choice(most_usual_values, size=len(model1), p=probabilities)\n",
    "\n",
    "# # Create a new DataFrame for the submission\n",
    "# submission_model2 = pd.DataFrame({\n",
    "#     'Claim Identifier': model1['Claim Identifier'],\n",
    "#     'Claim Injury Type': random_guesses\n",
    "# })\n",
    "\n",
    "# # Map the values in submission_model2 'Claim Injury Type' to match the original target values in label_mapping\n",
    "# submission_model2['Claim Injury Type'] = submission_model2['Claim Injury Type'].map(label_mapping)\n",
    "# print(submission_model2)\n",
    "\n",
    "\n",
    "# # Save the new submission file\n",
    "# submission_model2.to_csv('model3.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Things to  consider when optimizing models in deliverable 2\n",
    "\n",
    "\n",
    "\n",
    " ## Add Average Weekly Wage back with proper null value imputations\n",
    "\n",
    "\n",
    "\n",
    " ## These columns show low correlation with the target variable. Consider training models with and without them to see if they affect accuracy, precision and f1-score\n",
    "\n",
    " * District Name\n",
    "\n",
    " * Gender\n",
    "\n",
    " * Carrier type\n",
    "\n",
    " * Number of Dependents\n",
    "\n",
    "\n",
    "\n",
    " ## These columns showed high correlation between each other. Consider dropping one or two of them\n",
    "\n",
    " * WCIO Cause of Injury Code, WCIO Nature of Injury Code and WCIO Part Of Body Code had high correlations between each other AND nice correlations between target variable.\n",
    "\n",
    "     * We can consider droppig one or two of these to avoid redundancy\n",
    "\n",
    "\n",
    "\n",
    " ## Scaling\n",
    "\n",
    " * Try to also change z-score scaling to min-max scaling and see if it affects the accuracy of the model\n",
    "\n",
    " * Experience with scaling only numerical values and both numerical and categorial values\n",
    "\n",
    "\n",
    "\n",
    " ## Outliers\n",
    "\n",
    " * See if thresholding the days until injury reported -feature's values to even lower max-value affects the model accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ## Keep in mind, the unbalance of the target variable categories. If model performance is not good, try oversampling the rows with minority Claim Injury Type values or try SMOTE\n",
    "\n",
    " Claim Injury Type\n",
    "\n",
    " - 2. NON-COMP        0.510175\n",
    "\n",
    " - 4. TEMPORARY       0.257070\n",
    "\n",
    " - 3. MED ONLY        0.121362\n",
    "\n",
    " - 5. PPD SCH LOSS    0.084481\n",
    "\n",
    " - 1. CANCELLED       0.018900\n",
    "\n",
    " - 6. PPD NSL         0.007112\n",
    "\n",
    " - 8. DEATH           0.000775\n",
    "\n",
    " - 7. PTD             0.000126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # OPTIMIZATIONS ON KNNCLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#NOTE / TODO: START FROM PCA ANALYSIS WHAT WAS DONE IN DM LAB 08!! CHOOSE HOW MANY PCA's TO KEEP\n",
    "\n",
    "# Some heuristics (rules of thumb):\n",
    "\n",
    "# Reference: **Jolliffe, I. T. (2002). Principal component analysis.**\n",
    "\n",
    "# Section 6.1: *How Many Principal Components?*\n",
    "\n",
    "# - 6.1.1. Cumulative percentage of total variance\n",
    "#     - Keep 70-80% of total variance\n",
    "# - 6.1.2. Kaiser's rule\n",
    "#     - Keep PC with variance >= 1\n",
    "# - 6.1.3. Scree plot\n",
    "    # - Use elbow method\n",
    "\n",
    "\n",
    "# FROM DM LAB 08\n",
    "\n",
    "# # Get PCA output as table\n",
    "\n",
    "# # Get the eigenvalues (explained variance)\n",
    "# explained_variance = pca.explained_variance_\n",
    "\n",
    "# # Get the explained variance ratio\n",
    "# explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# # get the cumulative explained variance ratio \n",
    "# # Hint: use np.cumsum()\n",
    "# cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# # Combine into a dataframe\n",
    "# pca_results = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Eigenvalue\": explained_variance,\n",
    "#         \"Difference\": np.insert(np.diff(explained_variance), 0, 0),\n",
    "#         \"Proportion\": explained_variance_ratio,\n",
    "#         \"Cumulative\": cumulative_explained_variance_ratio\n",
    "#     },\n",
    "#         index=range(1, pca.n_components_ + 1)\n",
    "# )\n",
    "\n",
    "# pca_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DEEP NEURAL NETWORK MODEL TRAINING AND OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "train_data_NN = train_data_standardized.copy()\n",
    "test_data_NN = test_data_standardized.copy()\n",
    "\n",
    "# Prepare the data\n",
    "predictor_cols = train_data_NN.columns[train_data_NN.columns != \"Claim Injury Type\"]\n",
    "X = train_data_NN[predictor_cols]\n",
    "y = train_data_NN[\"Claim Injury Type\"]\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=5, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Define the total number of features in the dataset\n",
    "num_features = X.shape[1]\n",
    "\n",
    "# Lists to store the results for plotting\n",
    "accuracy_scores_train = []\n",
    "accuracy_scores_val = []\n",
    "f1_scores_val = []\n",
    "num_feature_list = list(range(1, num_features + 1))\n",
    "\n",
    "# Use LogisticRegression as the base estimator for RFE\n",
    "base_estimator = LogisticRegression(max_iter=1000)  \n",
    "\n",
    "# Loop over feature numbers from 1 to the total number of features\n",
    "for n_features in num_feature_list:\n",
    "    # Use RFE to select n_features\n",
    "    rfe = RFE(estimator=base_estimator, n_features_to_select=n_features)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features = rfe.support_\n",
    "\n",
    "    # Select the features from training and validation sets\n",
    "    X_train_rfe = X_train[:, selected_features]\n",
    "    X_val_rfe = X_val[:, selected_features]\n",
    "    \n",
    "    # Build the MLPClassifier with a deep structure and small hidden layers\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(32, 16, 16, 8, 8, 4),  # Deep structure with gradually decreasing sizes\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=110  # Increase this if convergence warnings appear (UPDATE: THIS WAS 90 AND I GOT CONVERGING ERRORS)\n",
    "    )\n",
    "\n",
    "    # Train the MLP model\n",
    "    mlp.fit(X_train_rfe, y_train)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    train_preds = mlp.predict(X_train_rfe)\n",
    "    accuracy_train = accuracy_score(y_train, train_preds)\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    val_preds = mlp.predict(X_val_rfe)\n",
    "    accuracy_val = accuracy_score(y_val, val_preds)\n",
    "    f1_val = f1_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "    # Store results\n",
    "    accuracy_scores_train.append(accuracy_train)\n",
    "    accuracy_scores_val.append(accuracy_val)\n",
    "    f1_scores_val.append(f1_val)\n",
    "\n",
    "    # Get selected feature names\n",
    "    selected_features_names = X.columns[selected_features]\n",
    "    # Print results for each feature set\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(\"Selected features: \", selected_features_names)\n",
    "    print(f\"Training Accuracy: {accuracy_train}\")\n",
    "    print(f\"Validation Accuracy: {accuracy_val}\")\n",
    "    print(f\"Validation F1 Score: {f1_val}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Plotting accuracy and F1 scores for different feature counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_feature_list, accuracy_scores_train, label='Training Accuracy', marker='o')\n",
    "plt.plot(num_feature_list, accuracy_scores_val, label='Validation Accuracy', marker='x')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy vs. Number of Features')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Validation F1 Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_feature_list, f1_scores_val, label='Validation F1 Score', marker='^')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Validation F1 Score vs. Number of Features')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## NN Model\n",
    "\n",
    "\n",
    "\n",
    " ### Train two models with two most promising set of features and and make predictions to test data with them\n",
    "\n",
    " * Set of features chosen are:\n",
    "\n",
    "     * 8 features (this far, almost all (expect one) features have been )\n",
    "\n",
    "         * Selected features:  Index(['Accident Date', 'Assembly Date', 'Attorney/Representative',\n",
    "\n",
    "        'Average Weekly Wage', 'C-2 Date', 'IME-4 Count',\n",
    "\n",
    "        'WCIO Nature of Injury Code', 'WCIO Part Of Body Code'],\n",
    "\n",
    "       dtype='object')\n",
    "\n",
    "         * Training Accuracy: 0.7463289006068101\n",
    "\n",
    "         * Validation Accuracy: 0.7439138195029581\n",
    "\n",
    "         * Validation F1 Score: 0.689082918712831\n",
    "\n",
    "     * 11 features (no further improvement worth mentioning in accuracy nor f1-score)\n",
    "\n",
    "         * Selected features:  Index(['Accident Date', 'Age at Injury', 'Assembly Date',\n",
    "\n",
    "        'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date', 'Gender',\n",
    "\n",
    "        'IME-4 Count', 'Industry Code', 'WCIO Nature of Injury Code',\n",
    "\n",
    "        'WCIO Part Of Body Code'],\n",
    "\n",
    "         dtype='object')\n",
    "\n",
    "         * Training Accuracy: 0.76391031260122\n",
    "\n",
    "         * Validation Accuracy: 0.761227836517989\n",
    "\n",
    "         * Validation F1 Score: 0.7046574402209247\n",
    "\n",
    " * NOTE: We didn't try 4 features yet, even though it looked promising, since two of them would have been date columns. Later features are assessed again with focus on other than date columns\n",
    "\n",
    " * These feature sets were chosen based on highest validation accuracy and highest f1-score\n",
    "\n",
    " * For all feature sets, validation and testing data accuracy are very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Build model with the optimal number of features\n",
    "\n",
    "train_data_NN = train_data_non_standardized.copy()\n",
    "test_data_NN = test_data_non_standardized.copy()\n",
    "\n",
    "# Prepare the data (NOTE: Object names are wrong and misleading here, since after adding more features, the optimal number of features changed)\n",
    "\n",
    "# THIS predictor_cols_13 IS ACTUALLY SET OF 11 FEATURES\n",
    "predictor_cols_13 = ['Accident Date', 'Age at Injury', 'Assembly Date', 'Attorney/Representative', 'Average Weekly Wage', 'C-2 Date', 'Gender', 'IME-4 Count', 'Industry Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "\n",
    "# THIS predictor_cols_10 IS ACTUALLY SET OF 8 FEATURES\n",
    "predictor_cols_10 = ['Accident Date', 'Assembly Date', 'Attorney/Representative','Average Weekly Wage', 'C-2 Date', 'IME-4 Count','WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "\n",
    "X_13 = train_data_NN[predictor_cols_13]\n",
    "X_10 = train_data_NN[predictor_cols_10]\n",
    "\n",
    "y_13 = train_data_NN[\"Claim Injury Type\"]\n",
    "y_10 = train_data_NN[\"Claim Injury Type\"]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train_13, X_val_13, y_train_13, y_val_13 = train_test_split(X_13, y_13, test_size=0.2, random_state=5, stratify=y_13)\n",
    "X_train_10, X_val_10, y_train_10, y_val_10 = train_test_split(X_10, y_10, test_size=0.2, random_state=5, stratify=y_10)\n",
    "\n",
    "# Standardize the data\n",
    "scaler_13 = StandardScaler()\n",
    "scaler_10 = StandardScaler()\n",
    "\n",
    "X_train_13 = scaler_13.fit_transform(X_train_13)\n",
    "X_val_13 = scaler_13.transform(X_val_13)\n",
    "test_data_NN_13 = test_data_NN[predictor_cols_13]\n",
    "# Scale the test data\n",
    "test_data_NN_13 = scaler_13.transform(test_data_NN_13)\n",
    "\n",
    "X_train_10 = scaler_10.fit_transform(X_train_10)\n",
    "X_val_10 = scaler_10.transform(X_val_10)\n",
    "test_data_NN_10 = test_data_NN[predictor_cols_10]\n",
    "# Scale the test data\n",
    "test_data_NN_10 = scaler_10.transform(test_data_NN_10)\n",
    "\n",
    "# Build the MLPClassifier with a deep structure and small hidden layers\n",
    "nn_model_10_features = MLPClassifier(\n",
    "    hidden_layer_sizes=(32, 16, 16, 8, 8, 4),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=150 \n",
    ")\n",
    "\n",
    "nn_model_13_features = MLPClassifier(\n",
    "    hidden_layer_sizes=(32, 16, 16, 8, 8, 4), \n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=150\n",
    ")\n",
    "\n",
    "\n",
    "# Train the MLP models\n",
    "nn_model_10_features.fit(X_train_10, y_train_10)\n",
    "nn_model_13_features.fit(X_train_13, y_train_13)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_preds_10 = nn_model_10_features.predict(X_train_10)\n",
    "accuracy_train_10 = accuracy_score(y_train_10, train_preds_10)\n",
    "print(f\"Training Accuracy with 8 features: {accuracy_train_10}\")\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_preds_10 = nn_model_10_features.predict(X_val_10)\n",
    "accuracy_val_10 = accuracy_score(y_val_10, val_preds_10)\n",
    "f1_val_10 = f1_score(y_val_10, val_preds_10, average='weighted')\n",
    "print(f\"Validation Accuracy with 8 features: {accuracy_val_10}\")\n",
    "print(f\"Validation F1 Score with 8 features: {f1_val_10}\")\n",
    "\n",
    "# Evaluate precision on validation data\n",
    "precision_val_10 = precision_score(y_val_10, val_preds_10, average='weighted')\n",
    "print(f\"Validation Precision Score with 8 features: {precision_val_10}\")\n",
    "\n",
    "# Same for 13 features\n",
    "train_preds_13 = nn_model_13_features.predict(X_train_13)\n",
    "accuracy_train_13 = accuracy_score(y_train_13, train_preds_13)\n",
    "print(f\"Training Accuracy with 11 features: {accuracy_train_13}\")\n",
    "\n",
    "val_preds_13 = nn_model_13_features.predict(X_val_13)\n",
    "accuracy_val_13 = accuracy_score(y_val_13, val_preds_13)\n",
    "f1_val_13 = f1_score(y_val_13, val_preds_13, average='weighted')\n",
    "print(f\"Validation Accuracy with 11 features: {accuracy_val_13}\")\n",
    "print(f\"Validation F1 Score with 11 features: {f1_val_13}\")\n",
    "\n",
    "precision_val_13 = precision_score(y_val_13, val_preds_13, average='weighted')\n",
    "print(f\"Validation Precision Score with 11 features: {precision_val_13}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save models\n",
    "\n",
    "# Save the models\n",
    "joblib.dump(nn_model_10_features, 'nn_model_8_features.pkl')\n",
    "joblib.dump(nn_model_13_features, 'nn_model_11_features.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get predictions for the test data with both models\n",
    "test_preds_10 = nn_model_10_features.predict(test_data_NN_10)\n",
    "test_preds_13 = nn_model_13_features.predict(test_data_NN_13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# See the unique values in the predictions\n",
    "unique_values = np.unique(test_preds_10)\n",
    "print(unique_values)\n",
    "\n",
    "unique_values_13 = np.unique(test_preds_13)\n",
    "print(unique_values_13)\n",
    "\n",
    "# Format the predictions according to the required label format\n",
    "formatted_predictions_10 = [claim_injury_type_mapping[pred] for pred in test_preds_10]\n",
    "formatted_predictions_13 = [claim_injury_type_mapping[pred] for pred in test_preds_13]\n",
    "\n",
    "\n",
    "# Create the submission DataFrames\n",
    "submission_10 = pd.DataFrame({\n",
    "    \"Claim Identifier\": test_data.index,\n",
    "    \"Claim Injury Type\": formatted_predictions_10\n",
    "})\n",
    "\n",
    "submission_13 = pd.DataFrame({\n",
    "    \"Claim Identifier\": test_data.index,\n",
    "    \"Claim Injury Type\": formatted_predictions_13\n",
    "})\n",
    "\n",
    "\n",
    "# Save to CSV in the required format\n",
    "submission_10.to_csv(\"Group39__Version10.csv\", index=False)\n",
    "submission_13.to_csv(\"Group39__Version11.csv\", index=False)\n",
    "\n",
    "# Check length of the submission files\n",
    "print(len(submission_10))\n",
    "print(len(submission_13))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Result in kaggle.\n",
    "\n",
    " * 8 features: 0.30994\n",
    "\n",
    " * 10 features: 0.19609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Further NN Model Experiments: Feature Selection re-visited:\n",
    "\n",
    " * Train model inside RFE:\n",
    "\n",
    "     * Without Accident Date and Assembly date, since their difference is already presented in Days Untill Injury reported column\n",
    "\n",
    "         * We will try this, since we don't think that the day of the injury itself or Assembly date alone are very effective predictors for the inury type of the claim\n",
    "\n",
    "     * We will use more iterations for NN model, since er didn't fully converge after 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "train_data_NN = train_data_non_standardized.copy()\n",
    "test_data_NN = test_data_non_standardized.copy()\n",
    "\n",
    "# Prepare the data\n",
    "predictor_cols = train_data_NN.columns.difference([\"Claim Injury Type\", \"Accident Date\", \"Assembly Date\"])\n",
    "print(predictor_cols)\n",
    "\n",
    "X = train_data_NN[predictor_cols]\n",
    "y = train_data_NN[\"Claim Injury Type\"]\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=5, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Define the total number of features in the dataset\n",
    "num_features = X.shape[1]\n",
    "\n",
    "# Lists to store the results for plotting\n",
    "accuracy_scores_train = []\n",
    "accuracy_scores_val = []\n",
    "f1_scores_val = []\n",
    "precision_val = []\n",
    "num_feature_list = list(range(1, num_features + 1))\n",
    "\n",
    "# Use LogisticRegression as the base estimator for RFE\n",
    "base_estimator = LogisticRegression(max_iter=1000)  \n",
    "\n",
    "# Loop over feature numbers from 1 to the total number of features\n",
    "for n_features in num_feature_list:\n",
    "    # Use RFE to select n_features\n",
    "    rfe = RFE(estimator=base_estimator, n_features_to_select=n_features)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features = rfe.support_\n",
    "\n",
    "    # Select the features from training and validation sets\n",
    "    X_train_rfe = X_train[:, selected_features]\n",
    "    X_val_rfe = X_val[:, selected_features]\n",
    "    \n",
    "    # Build the MLPClassifier with a deep structure and small hidden layers\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(32, 16, 16, 8, 8, 4),  # Deep structure with gradually decreasing sizes\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=200\n",
    "    )\n",
    "\n",
    "    # Train the MLP model\n",
    "    mlp.fit(X_train_rfe, y_train)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    train_preds = mlp.predict(X_train_rfe)\n",
    "    accuracy_train = accuracy_score(y_train, train_preds)\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    val_preds = mlp.predict(X_val_rfe)\n",
    "    accuracy_val = accuracy_score(y_val, val_preds)\n",
    "    f1_val = f1_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "    # Evaluate precision on validation data\n",
    "    precision_on_val = precision_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "    # Store results\n",
    "    accuracy_scores_train.append(accuracy_train)\n",
    "    accuracy_scores_val.append(accuracy_val)\n",
    "    f1_scores_val.append(f1_val)\n",
    "    precision_val.append(precision_on_val)\n",
    "\n",
    "    # Get selected feature names\n",
    "    selected_features_names = X.columns[selected_features]\n",
    "    # Print results for each feature set\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(\"Selected features: \", selected_features_names)\n",
    "    print(f\"Training Accuracy: {accuracy_train}\")\n",
    "    print(f\"Validation Accuracy: {accuracy_val}\")\n",
    "    print(f\"Validation F1 Score: {f1_val}\")\n",
    "    print(f\"Validation Precision Score: {precision_on_val}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Plotting accuracy and F1 scores for different feature counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_feature_list, accuracy_scores_train, label='Training Accuracy', marker='o')\n",
    "plt.plot(num_feature_list, accuracy_scores_val, label='Validation Accuracy', marker='x')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy vs. Number of Features')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Validation F1 Score and Precision\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_feature_list, f1_scores_val, label='Validation F1 Score', marker='^')\n",
    "plt.plot(num_feature_list, precision_val, label='Validation Precision', marker='s')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation F1 Score and Precision vs. Number of Features')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # JÄIT TÄHÄN WAGEN KANSSA!!\n",
    "\n",
    "\n",
    "\n",
    " ## - JATKA\n",
    "\n",
    " ## - JATKA\n",
    "\n",
    " ## - JATKA\n",
    "\n",
    " ## - JATKA\n",
    "\n",
    " ## - JATKA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Try two most promising set of features and make Kaggle returns with them\n",
    "\n",
    " ### Optimal number of features is now .... Let's build the model with a bit more max iterations for this set of ... features, make predictions for test data and let's try kaggle return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Build model with the optimal number of features\n",
    "\n",
    "train_data_NN = train_data_non_standardized.copy()\n",
    "test_data_NN = test_data_non_standardized.copy()\n",
    "\n",
    "# Prepare the data\n",
    "predictor_cols_11 = ['Age at Injury', 'Attorney/Representative', 'C-2 Date', 'Carrier Type', 'Days Until Injury Reported', 'Gender', 'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code','WCIO Part Of Body Code']\n",
    "\n",
    "X_11 = train_data_NN[predictor_cols_11]\n",
    "\n",
    "y_11 = train_data_NN[\"Claim Injury Type\"]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train_11, X_val_11, y_train_11, y_val_11 = train_test_split(X_11, y_11, test_size=0.2, random_state=5, stratify=y_11)\n",
    "\n",
    "# Standardize the data\n",
    "scaler_11 = StandardScaler()\n",
    "\n",
    "X_train_11 = scaler_11.fit_transform(X_train_11)\n",
    "X_val_11 = scaler_11.transform(X_val_11)\n",
    "test_data_NN_11 = test_data_NN[predictor_cols_11]\n",
    "# Scale the test data\n",
    "test_data_NN_11 = scaler_11.transform(test_data_NN_11)\n",
    "\n",
    "\n",
    "nn_model_11_features = MLPClassifier(\n",
    "    hidden_layer_sizes=(32, 16, 16, 8, 8, 4), \n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=200  \n",
    ")\n",
    "\n",
    "\n",
    "# Train the MLP model\n",
    "nn_model_11_features.fit(X_train_11, y_train_11)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_preds_11 = nn_model_11_features.predict(X_train_11)\n",
    "accuracy_train_11 = accuracy_score(y_train_11, train_preds_11)\n",
    "print(f\"Training Accuracy with 11 features (Assembly Date and Accident Date excluded): {accuracy_train_11}\")\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_preds_11 = nn_model_11_features.predict(X_val_11)\n",
    "accuracy_val_11 = accuracy_score(y_val_11, val_preds_11)\n",
    "f1_val_11 = f1_score(y_val_11, val_preds_11, average='weighted')\n",
    "print(f\"Validation Accuracy with 11 features (Assembly Date and Accident Date excluded): {accuracy_val_11}\")\n",
    "print(f\"Validation F1 Score with 11 features (Assembly Date and Accident Date excluded): {f1_val_11}\")\n",
    "\n",
    "# Evaluate precision on validation data\n",
    "precision_val_11 = precision_score(y_val_11, val_preds_11, average='weighted')\n",
    "print(f\"Validation Precision Score with 11 features (Assembly Date and Accident Date excluded): {precision_val_11}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save models\n",
    "\n",
    "# Save the models\n",
    "joblib.dump(nn_model_11_features, 'nn_model_11_features.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get predictions for the test data with both models\n",
    "test_preds_11 = nn_model_11_features.predict(test_data_NN_11)\n",
    "\n",
    "# See the unique values in the predictions\n",
    "unique_values = np.unique(test_preds_11)\n",
    "print(unique_values)\n",
    "\n",
    "# Format the predictions according to the required label format\n",
    "formatted_predictions_11 = [claim_injury_type_mapping[pred] for pred in test_preds_11]\n",
    "\n",
    "# Create the submission DataFrames\n",
    "submission_11 = pd.DataFrame({\n",
    "    \"Claim Identifier\": test_data.index,\n",
    "    \"Claim Injury Type\": formatted_predictions_11\n",
    "})\n",
    "\n",
    "\n",
    "# Save to CSV in the required format\n",
    "submission_11.to_csv(\"Group39__Version11.csv\", index=False)\n",
    "\n",
    "# Check length of the submission files\n",
    "print(len(submission_11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Result: 0.275 f1 score in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## OUTDATED!: (CODE ABOUT SMOTE COMMENTED OUT AND TRIED AFTER IMPORTANT FIXING OF PREPROCESSING )Further NN Model Experiments: Target value imbalance\n",
    "\n",
    " * Train model inside RFE and then with 3 best sets of features found this far (according to local environment assessment as well as Kaggle competition f1 score)\n",
    "\n",
    "     * Before training, apply Oversampling, Undersampling or SMOTE Techniques to get more minority result class values to the training. Our model is probably not good in predicting for those minoriity target variable value classes.\n",
    "\n",
    " * Training iterations of predictor features:\n",
    "\n",
    "     * Inside RFE to find best set of features:\n",
    "\n",
    "         * Inside RFE without assembly and accident date\n",
    "\n",
    "     * Predict for two best sets from both RFE procesure above\n",
    "\n",
    "     * Best 3 sets of previous experiments:\n",
    "\n",
    "         * See if modifications to SMOTE are needed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Chosen method: Creating synthetic minority class data with SMOTE\n",
    "\n",
    " * We chose SMOTE and not oversampling, since duplicating minority class members a lot, we might add risk for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Check the target value counts in train data\n",
    "# train_data_balanced = train_data_non_standardized.copy()\n",
    "\n",
    "\n",
    "# train_data_balanced['Claim Injury Type'].value_counts()\n",
    "\n",
    "\n",
    "# # Prepare the data\n",
    "# predictor_cols = train_data_balanced.columns[train_data_balanced.columns != \"Claim Injury Type\"]\n",
    "# X = train_data_balanced[predictor_cols]\n",
    "# y = train_data_balanced[\"Claim Injury Type\"]\n",
    "\n",
    "# # Scale the data\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# # We set a custom sampling strategy to reduce the amount of created syntetic samples and to keep the size of dataset OK \n",
    "# # From value count we saw that Majority class has 282253 samples. For minority classes that have less than 25% of the majority class samples are 3,5,1,6,8 and 7. For those classes we will set the sampling strategy to 0.25\n",
    "# # So we are adding synthetic samples to minority classes to have 25% of the majority class samples\n",
    "\n",
    "# \"\"\"\n",
    "# value counts\n",
    "# 2    282253\n",
    "# 4    141022\n",
    "# 3     67250\n",
    "# 5     46867\n",
    "# 1     10058\n",
    "# 6      3945\n",
    "# 8       431\n",
    "# 7        69\n",
    "# \"\"\"\n",
    "\n",
    "# target_size = int(0.25 * 282253)\n",
    "\n",
    "# # Define sampling strategy as a dictionary\n",
    "# # Only include classes with fewer samples than target_size (70563). \n",
    "# sampling_strategy = {\n",
    "#     3: target_size,  # Currently 67250 -> Increase to 70563\n",
    "#     5: target_size,  # Currently 46867 -> Increase to 70563\n",
    "#     1: target_size,  # Currently 10058 -> Increase to 70563\n",
    "#     6: target_size,  # Currently 3945 -> Increase to 70563\n",
    "#     7: target_size,  # Currently 69 -> Increase to 70563\n",
    "#     8: target_size,  # Currently 431 -> Increase to 70563\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# # Apply SMOTE \n",
    "# smote = SMOTE(sampling_strategy=sampling_strategy, random_state=5)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# # Convert the resampled data back to a DataFrame\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# y_resampled_df = pd.DataFrame(y_resampled, columns=['Claim Injury Type'])\n",
    "# train_data_resampled = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "\n",
    "# # Check for lenght of data and value counts in the resampled data\n",
    "# print(len(train_data_resampled))\n",
    "# train_data_resampled['Claim Injury Type'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Training with new balanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # RFE WITH ALL FEATURES (for SMOTE'd data)\n",
    "\n",
    "# train_data_NN = train_data_resampled.copy()\n",
    "# test_data_NN  = test_data_non_standardized.copy()\n",
    "\n",
    "\n",
    "# # Prepare the data\n",
    "# predictor_cols = train_data_NN.columns.difference([\"Claim Injury Type\", \"Accident Date\", \"Assembly Date\"])\n",
    "# X = train_data_NN[predictor_cols]\n",
    "# y = train_data_NN[\"Claim Injury Type\"]\n",
    "\n",
    "\n",
    "# # Split data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=5, stratify=y)\n",
    "\n",
    "# # Define the total number of features in the dataset\n",
    "# num_features = X.shape[1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# # Lists to store the results for plotting\n",
    "# accuracy_scores_train = []\n",
    "# accuracy_scores_val = []\n",
    "# f1_scores_val = []\n",
    "# num_feature_list = list(range(1, num_features + 1))\n",
    "\n",
    "# # Use LogisticRegression as the base estimator for RFE\n",
    "# base_estimator = LogisticRegression(max_iter=1000)  \n",
    "\n",
    "# # Loop over feature numbers from 1 to the total number of features\n",
    "# for n_features in num_feature_list:\n",
    "#     # Use RFE to select n_features\n",
    "#     rfe = RFE(estimator=base_estimator, n_features_to_select=n_features)\n",
    "#     rfe.fit(X_train, y_train)\n",
    "#     selected_features = rfe.support_\n",
    "\n",
    "#     #NOTE: THIS FOR NUMPY!\n",
    "#     # Select the features from training and validation sets\n",
    "#     # X_train_rfe = X_train[:, selected_features]\n",
    "#     # X_val_rfe = X_val[:, selected_features]\n",
    "\n",
    "#     #NOTE: THIS FOR DATAFRAMES!\n",
    "#     selected_columns = X_train.columns[selected_features] # Get the selected column names\n",
    "#     X_train_rfe = X_train[selected_columns]\n",
    "#     X_val_rfe = X_val[selected_columns]\n",
    "\n",
    "    \n",
    "#     # Build the MLPClassifier with a deep structure and small hidden layers\n",
    "#     mlp = MLPClassifier(\n",
    "#         hidden_layer_sizes=(32, 16, 16, 8, 8, 4),  # Deep structure with gradually decreasing sizes\n",
    "#         activation='relu',\n",
    "#         solver='adam',\n",
    "#         max_iter=200  \n",
    "#     )\n",
    "\n",
    "#     # Train the MLP model\n",
    "#     mlp.fit(X_train_rfe, y_train)\n",
    "\n",
    "#     # Evaluate on training data\n",
    "#     train_preds = mlp.predict(X_train_rfe)\n",
    "#     accuracy_train = accuracy_score(y_train, train_preds)\n",
    "\n",
    "#     # Evaluate on validation data\n",
    "#     val_preds = mlp.predict(X_val_rfe)\n",
    "#     accuracy_val = accuracy_score(y_val, val_preds)\n",
    "#     f1_val = f1_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "#     # Store results\n",
    "#     accuracy_scores_train.append(accuracy_train)\n",
    "#     accuracy_scores_val.append(accuracy_val)\n",
    "#     f1_scores_val.append(f1_val)\n",
    "\n",
    "#     # Get selected feature names\n",
    "#     selected_features_names = selected_columns\n",
    "#     # Print results for each feature set\n",
    "#     print(f\"Number of features: {n_features}\")\n",
    "#     print(\"Selected features: \", selected_features_names)\n",
    "#     print(f\"Training Accuracy: {accuracy_train}\")\n",
    "#     print(f\"Validation Accuracy: {accuracy_val}\")\n",
    "#     print(f\"Validation F1 Score: {f1_val}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Plotting accuracy and F1 scores for different feature counts\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Plot Training and Validation Accuracy\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(num_feature_list, accuracy_scores_train, label='Training Accuracy', marker='o')\n",
    "# plt.plot(num_feature_list, accuracy_scores_val, label='Validation Accuracy', marker='x')\n",
    "# plt.xlabel('Number of Features')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Validation Accuracy vs. Number of Features')\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot Validation F1 Score\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(num_feature_list, f1_scores_val, label='Validation F1 Score', marker='^')\n",
    "# plt.xlabel('Number of Features')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('Validation F1 Score vs. Number of Features')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next we will train models with 4 most promising set of features with even more iterations allowed for the NN and save the model.\n",
    "\n",
    " * Set 1: Three features: ['Attorney/Representative', 'IME-4 Count', 'WCIO Nature of Injury Code']\n",
    "\n",
    "     * We want to explore this since it was the first set of features that pumped up the accuracy radically\n",
    "\n",
    " * Set 2: Nine features: ['Age at Injury', 'Attorney/Representative', 'C-2 Date', 'Days Until Injury Reported', 'Gender', 'IME-4 Count', 'Industry Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "\n",
    "     * We want to explore this since it was the set of features, which after, we didn't significally prove accuracy or f1-score\n",
    "\n",
    " * Set 3: Thirteen features: All but Assemly Date and Age at Injury\n",
    "\n",
    "     * We want to explore this since it was the set that produced overall highest accuracies and f1-scores\n",
    "\n",
    " * Set 4: All the features that improved training and validation accracy and f1-score:\n",
    "\n",
    "     * Selected features: ['Age at Injury', 'Attorney/Representative', 'C-2 Date', 'Days Until Injury Reported', 'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Carrier Type']\n",
    "\n",
    "     * We want to explore this since we want to try how the result comes up with only the features that improved the metrics from the previous iteration of RFE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Apply smote to all datasets\n",
    "\n",
    "# # Check the target value counts in train data\n",
    "# train_data_balanced = train_data_non_standardized.copy()\n",
    "# test_data_balanced = test_data_non_standardized.copy()\n",
    "\n",
    "# # Prepare the data\n",
    "# predictor_cols_all = train_data_NN.columns.difference([\"Claim Injury Type\", \"Accident Date\", \"Assembly Date\"])\n",
    "# predictor_cols_3 = ['Attorney/Representative', 'IME-4 Count', 'WCIO Nature of Injury Code'] \n",
    "# predictor_cols_9 = ['Age at Injury', 'Attorney/Representative', 'C-2 Date', 'Days Until Injury Reported', 'Gender', 'IME-4 Count', 'Industry Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "# predictor_cols_improved = ['Age at Injury', 'Attorney/Representative', 'C-2 Date', 'Days Until Injury Reported', 'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Carrier Type']\n",
    "\n",
    "\n",
    "# X_all = train_data_NN[predictor_cols_all]\n",
    "# X_3 = train_data_NN[predictor_cols_3]\n",
    "# X_9 = train_data_NN[predictor_cols_9]\n",
    "# X_improved = train_data_NN[predictor_cols_improved]\n",
    "\n",
    "# y_all = train_data_NN[\"Claim Injury Type\"]\n",
    "# y_3 = train_data_NN[\"Claim Injury Type\"]\n",
    "# y_9 = train_data_NN[\"Claim Injury Type\"]\n",
    "# y_improved = train_data_NN[\"Claim Injury Type\"]\n",
    "\n",
    "\n",
    "\n",
    "# # Scale the data\n",
    "# scaler_all = StandardScaler()\n",
    "# scaler_3 = StandardScaler()\n",
    "# scaler_9 = StandardScaler()\n",
    "# scaler_improved = StandardScaler()\n",
    "\n",
    "# X_scaled_all = scaler_all.fit_transform(X_all)\n",
    "# X_scaled_3 = scaler_3.fit_transform(X_3)\n",
    "# X_scaled_9 = scaler_9.fit_transform(X_9)\n",
    "# X_scaled_improved = scaler_improved.fit_transform(X_improved)\n",
    "\n",
    "# # Prepeare test data\n",
    "# test_data_NN_all = test_data_NN[predictor_cols_all]\n",
    "# test_data_NN_3 = test_data_NN[predictor_cols_3]\n",
    "# test_data_NN_9 = test_data_NN[predictor_cols_9]\n",
    "# test_data_NN_improved = test_data_NN[predictor_cols_improved]\n",
    "\n",
    "# # Scale the test data\n",
    "# test_data_NN_all = scaler_all.transform(test_data_NN_all)\n",
    "# test_data_NN_3 = scaler_3.transform(test_data_NN_3)\n",
    "# test_data_NN_9 = scaler_9.transform(test_data_NN_9)\n",
    "# test_data_NN_improved = scaler_improved.transform(test_data_NN_improved)\n",
    "\n",
    "# # We set a custom sampling strategy to reduce the amount of created syntetic samples and to keep the size of dataset OK \n",
    "# # From value count we saw that Majority class has 282253 samples. For minority classes that have less than 25% of the majority class samples are 3,5,1,6,8 and 7. For those classes we will set the sampling strategy to 0.25\n",
    "# # So we are adding synthetic samples to minority classes to have 25% of the majority class samples\n",
    "\n",
    "# \"\"\"\n",
    "# value counts\n",
    "# 2    282253\n",
    "# 4    141022\n",
    "# 3     67250\n",
    "# 5     46867\n",
    "# 1     10058\n",
    "# 6      3945\n",
    "# 8       431\n",
    "# 7        69\n",
    "# \"\"\"\n",
    "\n",
    "# target_size = int(0.25 * 282253)\n",
    "\n",
    "# # Define sampling strategy as a dictionary\n",
    "# # Only include classes with fewer samples than target_size (70563). \n",
    "# sampling_strategy = {\n",
    "#     3: target_size,  # Currently 67250 -> Increase to 70563\n",
    "#     5: target_size,  # Currently 46867 -> Increase to 70563\n",
    "#     1: target_size,  # Currently 10058 -> Increase to 70563\n",
    "#     6: target_size,  # Currently 3945 -> Increase to 70563\n",
    "#     7: target_size,  # Currently 69 -> Increase to 70563\n",
    "#     8: target_size,  # Currently 431 -> Increase to 70563\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# # Apply SMOTE \n",
    "# smote_all = SMOTE(sampling_strategy=sampling_strategy, random_state=5)\n",
    "# smote_3 = SMOTE(sampling_strategy=sampling_strategy, random_state=5)\n",
    "# smote_9 = SMOTE(sampling_strategy=sampling_strategy, random_state=5)\n",
    "# smote_improved = SMOTE(sampling_strategy=sampling_strategy, random_state=5)\n",
    "\n",
    "# X_resampled_all, y_resampled_all = smote_all.fit_resample(X_scaled_all, y_all)\n",
    "# X_resampled_3, y_resampled_3 = smote_3.fit_resample(X_scaled_3, y_3)\n",
    "# X_resampled_9, y_resampled_9 = smote_9.fit_resample(X_scaled_9, y_9)\n",
    "# X_resampled_improved, y_resampled_improved = smote_improved.fit_resample(X_scaled_improved, y_improved)\n",
    "\n",
    "# # Convert the resampled data back to a DataFrame\n",
    "\n",
    "# X_resampled_all_df = pd.DataFrame(X_resampled_all, columns=X_all.columns)\n",
    "# y_resampled_all_df = pd.DataFrame(y_resampled_all, columns=['Claim Injury Type'])\n",
    "# train_data_resampled_all = pd.concat([X_resampled_all_df, y_resampled_all_df], axis=1)\n",
    "\n",
    "# X_resampled_3_df = pd.DataFrame(X_resampled_3, columns=X_3.columns)\n",
    "# y_resampled_3_df = pd.DataFrame(y_resampled_3, columns=['Claim Injury Type'])\n",
    "# train_data_resampled_3 = pd.concat([X_resampled_3_df, y_resampled_3_df], axis=1)\n",
    "\n",
    "# X_resampled_9_df = pd.DataFrame(X_resampled_9, columns=X_9.columns)\n",
    "# y_resampled_9_df = pd.DataFrame(y_resampled_9, columns=['Claim Injury Type'])\n",
    "# train_data_resampled_9 = pd.concat([X_resampled_9_df, y_resampled_9_df], axis=1)\n",
    "\n",
    "# X_resampled_improved_df = pd.DataFrame(X_resampled_improved, columns=X_improved.columns)\n",
    "# y_resampled_improved_df = pd.DataFrame(y_resampled_improved, columns=['Claim Injury Type'])\n",
    "# train_data_resampled_improved = pd.concat([X_resampled_improved_df, y_resampled_improved_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# # Check for lenght of data and value counts in the resampled data\n",
    "# print(len(train_data_resampled_all))\n",
    "# print(len(train_data_resampled_3))\n",
    "# print(len(train_data_resampled_9))\n",
    "# print(len(train_data_resampled_improved))\n",
    "\n",
    "# print(train_data_resampled_all['Claim Injury Type'].value_counts())\n",
    "# print(train_data_resampled_3['Claim Injury Type'].value_counts())\n",
    "# print(train_data_resampled_9['Claim Injury Type'].value_counts())\n",
    "# print(train_data_resampled_improved['Claim Injury Type'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Split data into training and validation sets\n",
    "# X_resampled_all_df = train_data_resampled_all[predictor_cols_all]\n",
    "# y_resampled_all_df = train_data_resampled_all[\"Claim Injury Type\"]\n",
    "# X_train_all, X_val_all, y_train_all, y_val_all = train_test_split(X_resampled_all_df, y_resampled_all_df, test_size=0.2, random_state=5, stratify=y_resampled_all_df)\n",
    "\n",
    "# X_resampled_3_df = train_data_resampled_3[predictor_cols_3]\n",
    "# y_resampled_3_df = train_data_resampled_3[\"Claim Injury Type\"]\n",
    "# X_train_3, X_val_3, y_train_3, y_val_3 = train_test_split(X_resampled_3_df, y_resampled_3_df, test_size=0.2, random_state=5, stratify=y_resampled_3_df)\n",
    "\n",
    "# X_resampled_9_df = train_data_resampled_9[predictor_cols_9]\n",
    "# y_resampled_9_df = train_data_resampled_9[\"Claim Injury Type\"]\n",
    "# X_train_9, X_val_9, y_train_9, y_val_9 = train_test_split(X_resampled_9_df, y_resampled_9_df, test_size=0.2, random_state=5, stratify=y_resampled_9_df)\n",
    "\n",
    "# X_resampled_improved_df = train_data_resampled_improved[predictor_cols_improved]\n",
    "# y_resampled_improved_df = train_data_resampled_improved[\"Claim Injury Type\"]\n",
    "# X_train_improved, X_val_improved, y_train_improved, y_val_improved = train_test_split(X_resampled_improved_df, y_resampled_improved_df, test_size=0.2, random_state=5, stratify=y_resampled_improved_df)\n",
    "\n",
    "\n",
    "# # Define the total number of features in the dataset\n",
    "# num_features_all = X_resampled_all_df.shape[1]\n",
    "# num_features_3 = X_resampled_3_df.shape[1]\n",
    "# num_features_9 = X_resampled_9_df.shape[1]\n",
    "# num_features_improved = X_resampled_improved_df.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Train model with all features with more iterations\n",
    "# nn_model_all_features = MLPClassifier(\n",
    "#     hidden_layer_sizes=(32, 16, 16, 8, 8, 4), \n",
    "#     activation='relu',\n",
    "#     solver='adam',\n",
    "#     max_iter=300  \n",
    "# )\n",
    "\n",
    "# # Train the MLP model\n",
    "# nn_model_all_features.fit(X_train_all, y_train_all)\n",
    "\n",
    "# # Evaluate on training data\n",
    "# train_preds_all = nn_model_all_features.predict(X_train_all)\n",
    "# accuracy_train_all = accuracy_score(y_train_all, train_preds_all)\n",
    "# print(f\"Training Accuracy with all features (Other than Assembly and Accident Date): {accuracy_train_all}\")\n",
    "\n",
    "# # Evaluate on validation data\n",
    "# val_preds_all = nn_model_all_features.predict(X_val_all)\n",
    "# accuracy_val_all = accuracy_score(y_val_all, val_preds_all)\n",
    "# f1_val_all = f1_score(y_val_all, val_preds_all, average='weighted')\n",
    "# print(f\"Validation Accuracy with all features (Other than Assembly and Accident Date): {accuracy_val_all}\")\n",
    "# print(f\"Validation F1 Score with all features (Other than Assembly and Accident Date): {f1_val_all}\")\n",
    "\n",
    "# # Evaluate precision on validation data\n",
    "# precision_val_all = precision_score(y_val_all, val_preds_all, average='weighted')\n",
    "# print(f\"Validation Precision Score with all features (Other than Assembly and Accident Date): {precision_val_all}\")\n",
    "\n",
    "# # Save the model\n",
    "# joblib.dump(nn_model_all_features, 'nn_model_all_features_smote.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Train model with 3 features with more iterations\n",
    "# nn_model_3_features = MLPClassifier(\n",
    "#     hidden_layer_sizes=(32, 16, 16, 8, 8, 4), \n",
    "#     activation='relu',\n",
    "#     solver='adam',\n",
    "#     max_iter=300  \n",
    "# )\n",
    "\n",
    "# # Train the MLP model\n",
    "# nn_model_3_features.fit(X_train_3, y_train_3)\n",
    "\n",
    "# # Evaluate on training data\n",
    "# train_preds_3 = nn_model_3_features.predict(X_train_3)\n",
    "# accuracy_train_3 = accuracy_score(y_train_3, train_preds_3)\n",
    "# print(f\"Training Accuracy with 3 features: {accuracy_train_3}\")\n",
    "\n",
    "# # Evaluate on validation data\n",
    "# val_preds_3 = nn_model_3_features.predict(X_val_3)\n",
    "# accuracy_val_3 = accuracy_score(y_val_3, val_preds_3)\n",
    "# f1_val_3 = f1_score(y_val_3, val_preds_3, average='weighted')\n",
    "# print(f\"Validation Accuracy with 3 features: {accuracy_val_3}\")\n",
    "# print(f\"Validation F1 Score with 3 features: {f1_val_3}\")\n",
    "\n",
    "# # Evaluate precision on validation data\n",
    "# precision_val_3 = precision_score(y_val_3, val_preds_3, average='weighted')\n",
    "# print(f\"Validation Precision Score with 3 features: {precision_val_3}\")\n",
    "\n",
    "# # Save model with 3 features\n",
    "# joblib.dump(nn_model_3_features, 'nn_model_3_features_smote.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Train model with 9 features with more iterations\n",
    "# nn_model_9_features = MLPClassifier(\n",
    "#     hidden_layer_sizes=(32, 16, 16, 8, 8, 4), \n",
    "#     activation='relu',\n",
    "#     solver='adam',\n",
    "#     max_iter=300  \n",
    "# )\n",
    "\n",
    "# # Train the MLP model\n",
    "# nn_model_9_features.fit(X_train_9, y_train_9)\n",
    "\n",
    "# # Evaluate on training data\n",
    "# train_preds_9 = nn_model_9_features.predict(X_train_9)\n",
    "# accuracy_train_9 = accuracy_score(y_train_9, train_preds_9)\n",
    "# print(f\"Training Accuracy with 9 features: {accuracy_train_9}\")\n",
    "\n",
    "# # Evaluate on validation data\n",
    "# val_preds_9 = nn_model_9_features.predict(X_val_9)\n",
    "# accuracy_val_9 = accuracy_score(y_val_9, val_preds_9)\n",
    "# f1_val_9 = f1_score(y_val_9, val_preds_9, average='weighted')\n",
    "\n",
    "# print(f\"Validation Accuracy with 9 features: {accuracy_val_9}\")\n",
    "# print(f\"Validation F1 Score with 9 features: {f1_val_9}\")\n",
    "\n",
    "# # Evaluate precision on validation data\n",
    "# precision_val_9 = precision_score(y_val_9, val_preds_9, average='weighted')\n",
    "# print(f\"Validation Precision Score with 9 features: {precision_val_9}\")\n",
    "\n",
    "# # Save model with 9 features\n",
    "# joblib.dump(nn_model_9_features, 'nn_model_9_features_smote.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Train model with improved features with more iterations\n",
    "# nn_model_improved_features = MLPClassifier(\n",
    "#     hidden_layer_sizes=(32, 16, 16, 8, 8, 4), \n",
    "#     activation='relu',\n",
    "#     solver='adam',\n",
    "#     max_iter=300  \n",
    "# )\n",
    "\n",
    "# # Train the MLP model\n",
    "# nn_model_improved_features.fit(X_train_improved, y_train_improved)\n",
    "\n",
    "# # Evaluate on training data\n",
    "# train_preds_improved = nn_model_improved_features.predict(X_train_improved)\n",
    "\n",
    "# accuracy_train_improved = accuracy_score(y_train_improved, train_preds_improved)\n",
    "# print(f\"Training Accuracy with improved features: {accuracy_train_improved}\")\n",
    "\n",
    "# # Evaluate on validation data\n",
    "# val_preds_improved = nn_model_improved_features.predict(X_val_improved)\n",
    "# accuracy_val_improved = accuracy_score(y_val_improved, val_preds_improved)\n",
    "# f1_val_improved = f1_score(y_val_improved, val_preds_improved, average='weighted')\n",
    "\n",
    "# print(f\"Validation Accuracy with improved features: {accuracy_val_improved}\")\n",
    "# print(f\"Validation F1 Score with improved features: {f1_val_improved}\")\n",
    "\n",
    "# # Evaluate precision on validation data\n",
    "# precision_val_improved = precision_score(y_val_improved, val_preds_improved, average='weighted')\n",
    "# print(f\"Validation Precision Score with improved features: {precision_val_improved}\")\n",
    "\n",
    "# # Save model with improved features\n",
    "# joblib.dump(nn_model_improved_features, 'nn_model_improved_features_smote.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# # Get predictions for the test data with all models\n",
    "# test_preds_all = nn_model_all_features.predict(test_data_NN_all)\n",
    "# test_preds_3 = nn_model_3_features.predict(test_data_NN_3)\n",
    "# test_preds_9 = nn_model_9_features.predict(test_data_NN_9)\n",
    "# test_preds_improved = nn_model_improved_features.predict(test_data_NN_improved)\n",
    "\n",
    "\n",
    "# # See the unique values in the predictions\n",
    "# unique_values_all = np.unique(test_preds_all)\n",
    "# print(unique_values_all)\n",
    "\n",
    "# unique_values_3 = np.unique(test_preds_3)\n",
    "# print(unique_values_3)\n",
    "\n",
    "# unique_values_9 = np.unique(test_preds_9)\n",
    "# print(unique_values_9)\n",
    "\n",
    "# unique_values_improved = np.unique(test_preds_improved)\n",
    "# print(unique_values_improved)\n",
    "\n",
    "# # Format the predictions according to the required label format\n",
    "# formatted_predictions_all = [claim_injury_type_mapping[pred] for pred in test_preds_all]\n",
    "# formatted_predictions_3 = [claim_injury_type_mapping[pred] for pred in test_preds_3]\n",
    "# formatted_predictions_9 = [claim_injury_type_mapping[pred] for pred in test_preds_9]\n",
    "# formatted_predictions_improved = [claim_injury_type_mapping[pred] for pred in test_preds_improved]\n",
    "\n",
    "# # Create the submission DataFrames\n",
    "# submission_all = pd.DataFrame({\n",
    "#     \"Claim Identifier\": test_data.index,\n",
    "#     \"Claim Injury Type\": formatted_predictions_all\n",
    "# })\n",
    "\n",
    "# submission_3 = pd.DataFrame({\n",
    "#     \"Claim Identifier\": test_data.index,\n",
    "#     \"Claim Injury Type\": formatted_predictions_3\n",
    "# })\n",
    "\n",
    "# submission_9 = pd.DataFrame({\n",
    "#     \"Claim Identifier\": test_data.index,\n",
    "#     \"Claim Injury Type\": formatted_predictions_9\n",
    "# })\n",
    "\n",
    "# submission_improved = pd.DataFrame({\n",
    "#     \"Claim Identifier\": test_data.index,\n",
    "#     \"Claim Injury Type\": formatted_predictions_improved\n",
    "# })\n",
    "\n",
    "# # Save to CSV in the required format\n",
    "# submission_all.to_csv(\"Group39__Version05.csv\", index=False)\n",
    "# submission_3.to_csv(\"Group39__Version06.csv\", index=False)\n",
    "# submission_9.to_csv(\"Group39__Version07.csv\", index=False)\n",
    "# submission_improved.to_csv(\"Group39__Version08.csv\", index=False)\n",
    "\n",
    "\n",
    "# # Check length of the submission files\n",
    "# print(len(submission_all))\n",
    "# print(len(submission_3))\n",
    "# print(len(submission_9))\n",
    "# print(len(submission_improved))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# #See the unique values in the submission files\n",
    "# print(submission_all['Claim Injury Type'].value_counts())\n",
    "# print(submission_3['Claim Injury Type'].value_counts())\n",
    "# print(submission_9['Claim Injury Type'].value_counts())\n",
    "# print(submission_improved['Claim Injury Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (POOR RESULTS AND CODE COMMENTED OUT)Results with Smote and RFE\n",
    "\n",
    " * 3 features\n",
    "\n",
    "     * Accuracy val data: 0.591\n",
    "\n",
    "     * f1-score val data: 0.547\n",
    "\n",
    "     * f1-score kaggle: 0.00...\n",
    "\n",
    "     * NOTE: ONLY MINORITY CLASSES IN PREDICTIONS!! We don't trust this set of features at all to be good predicctors for target **on their own**\n",
    "\n",
    " * 9 features\n",
    "\n",
    "     * Accuracy val data: 0.656\n",
    "\n",
    "     * f1-score val data: 0.612\n",
    "\n",
    "     * f1-score kaggle: 0.15\n",
    "\n",
    " * All features\n",
    "\n",
    "     * Accuracy val data: 0.670\n",
    "\n",
    "     * f1-score val data: 0.629\n",
    "\n",
    "     * f1-score kaggle: 0.13\n",
    "\n",
    " * 10 (\"improved\") features\n",
    "\n",
    "     * Accuracy val data: 0.665\n",
    "\n",
    "     * f1-score val data: 0.623\n",
    "\n",
    "     * f1-score kaggle:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Addition of Average Weekly Wage back to model\n",
    "\n",
    "\n",
    "\n",
    " * Team member (and befoe this line also we) found out that this makes the f1-score better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Further NN Model Experiment: Change RFE to Decision Tree RFE instead of Logistic Regression RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SMOTE/Oversampling tried again.\n",
    "\n",
    " * Last time (also last time we tried, we didn't have wage column and also we potentially had leaky data preprocessing process) we tried SMOTE, it wasn't successcull. After less leaky data preprocessing, experimenting with different RFE methods and hyperparameter tuning, lets\n",
    "\n",
    " * We still have minority class members underpresneted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## NN Model: Hyperparameter Tuning and other experiments\n",
    "\n",
    " * Try Robust scaler (from ML practicals: Neural Networks) for data instead of standard scaler\n",
    "\n",
    " * Hyperaparameter Tuning\n",
    "\n",
    "     * Layer tuning\n",
    "\n",
    "     * Solver tuning\n",
    "\n",
    "     * Max iteration tuning\n",
    "\n",
    "     * Other..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## NN Model: Assessment\n",
    "\n",
    " * See precision, recall, f1-score, training and testing accuracy and confusion matrix and assess the quality of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
